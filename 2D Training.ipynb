{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Random Seed: 236 ===========\n"
     ]
    }
   ],
   "source": [
    "from OurTrainingTools2D import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurCDModel(nn.Module):\n",
    "### Defines the  model with parametrized discriminant. Only quadratic dependence on a single parameter is implemented.\n",
    "### Input is the architecture (list of integers, the last one being equal to 1) and the activation type ('ReLU' or 'Sigmoid')\n",
    "    def __init__(self, NumberOfParameters, AR = [1, 3, 3, 1] , AF = 'ReLU'):               \n",
    "        super(OurCDModel, self).__init__() \n",
    "        ValidActivationFunctions = {'ReLU': torch.relu, 'Sigmoid': torch.sigmoid}\n",
    "        try:\n",
    "            self.ActivationFunction = ValidActivationFunctions[AF]\n",
    "        except KeyError:\n",
    "            print('The activation function specified is not valid. Allowed activations are %s.'\n",
    "                 %str(list(ValidActivationFunctions.keys())))\n",
    "            print('Will use ReLU.')\n",
    "            self.ActivationFunction = torch.relu            \n",
    "        if type(AR) == list:\n",
    "            if( ( all(isinstance(n, int) for n in AR)) and ( AR[-1] == 1) ):\n",
    "                self.Architecture = AR\n",
    "            else:\n",
    "                print('Architecture should be a list of integers, the last one should be 1.')\n",
    "                raise ValueError             \n",
    "        else:\n",
    "            print('Architecture should be a list !')\n",
    "            raise ValueError\n",
    "        self.NumberOfParameters = NumberOfParameters\n",
    "\n",
    "### Define Layers\n",
    "        self.NumberOfNetworks = int((2+NumberOfParameters)*(1+NumberOfParameters)/2)-1\n",
    "        LinearLayers = [([nn.Linear(self.Architecture[i], self.Architecture[i+1]) \\\n",
    "                                  for i in range(len(self.Architecture)-1)])\\\n",
    "                        for n in range(self.NumberOfNetworks)]\n",
    "        LinearLayers = [Layer for SubLayerList in LinearLayers for Layer in SubLayerList]\n",
    "        self.LinearLayers = nn.ModuleList(LinearLayers)\n",
    "        \n",
    "    def Forward(self, Data, Parameters):\n",
    "### Forward Function. Performs Preprocessing, returns F = rho/(1+rho) in [0,1], where rho is quadratically parametrized.\n",
    "        # Checking that data has the right input dimension\n",
    "        InputDimension = self.Architecture[0]\n",
    "        if Data.size(1) != InputDimension:\n",
    "            print('Dimensions of the data and the network input mismatch: data: %d, model: %d'\n",
    "                  %(Data.size(1), InputDimension))\n",
    "            raise ValueError\n",
    "\n",
    "        # Checking that preprocess has been initialised\n",
    "        if not hasattr(self, 'Shift'):\n",
    "            print('Please initialize preprocess parameters!')\n",
    "            raise ValueError\n",
    "        with torch.no_grad(): \n",
    "            Data, Parameters = self.Preprocess(Data, Parameters)  \n",
    "        \n",
    "        NumberOfLayers, NumberOfEvents = len(self.Architecture)-1, Data.size(0)\n",
    "        EntryIterator, NetworkIterator = 0, -1\n",
    "        MatrixLT = torch.zeros([NumberOfEvents, (self.NumberOfParameters+1)**2], dtype=Data.dtype)\n",
    "        \n",
    "        if Data.is_cuda:\n",
    "            MatrixLT = OurCudaTensor(MatrixLT)\n",
    "        \n",
    "        for i in range(self.NumberOfParameters+1):\n",
    "            EntryIterator += i\n",
    "            DiagonalEntry = True\n",
    "            for j in range(self.NumberOfParameters+1-i):\n",
    "                if NetworkIterator == -1:\n",
    "                    MatrixLT[:, EntryIterator] = torch.ones(NumberOfEvents)\n",
    "                    #print('Entry: %d, Layer: ones, DiagonalEntry: %s'%(EntryIterator,\n",
    "                    #                                                str(DiagonalEntry)))\n",
    "                else:\n",
    "                    x = Data\n",
    "                    for Layer in self.LinearLayers[NumberOfLayers*NetworkIterator:\\\n",
    "                                                  NumberOfLayers*(NetworkIterator+1)-1]:\n",
    "                        x = self.ActivationFunction(Layer(x))\n",
    "                    x = self.LinearLayers[NumberOfLayers*(NetworkIterator+1)-1](x).squeeze()\n",
    "                    MatrixLT[:, EntryIterator] = torch.exp(x) if DiagonalEntry else x\n",
    "                    #print('Entry: %d, Layer: %d, DiagonalEntry: %s'%(EntryIterator, NetworkIterator, \n",
    "                    #                                                str(DiagonalEntry)))\n",
    "                EntryIterator += 1\n",
    "                NetworkIterator += 1\n",
    "                DiagonalEntry = False\n",
    "        #print('MatrixLT: '+str(MatrixLT.is_cuda))\n",
    "        #print('Parameters: '+str(Parameters.is_cuda))\n",
    "\n",
    "        MatrixLT = MatrixLT.reshape([-1, self.NumberOfParameters+1, self.NumberOfParameters+1])\n",
    "        MatrixLTP = MatrixLT.matmul(Parameters.reshape([NumberOfEvents, self.NumberOfParameters+1, 1]))\n",
    "        rho = MatrixLTP.permute([0, 2, 1]).matmul(MatrixLTP).squeeze()\n",
    "        \n",
    "        return (rho.div(1.+rho)).view(-1, 1)\n",
    "    \n",
    "    def GetL1Bound(self, L1perUnit):\n",
    "        self.L1perUnit = L1perUnit\n",
    "    \n",
    "    def ClipL1Norm(self):\n",
    "### Clip the weights      \n",
    "        def ClipL1NormLayer(DesignatedL1Max, Layer, Counter):\n",
    "            if Counter == 1:\n",
    "                ### this avoids clipping the first layer\n",
    "                return\n",
    "            L1 = Layer.weight.abs().sum()\n",
    "            Layer.weight.masked_scatter_(L1 > DesignatedL1Max, \n",
    "                                        Layer.weight*(DesignatedL1Max/L1))\n",
    "            return\n",
    "        \n",
    "        Counter = 0\n",
    "        for m in self.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                Counter += 1\n",
    "                with torch.no_grad():\n",
    "                    DesignatedL1Max = m.weight.size(0)*m.weight.size(1)*self.L1perUnit\n",
    "                    ClipL1NormLayer(DesignatedL1Max, m, Counter)\n",
    "            else:\n",
    "                for mm in m:\n",
    "                    Counter +=1\n",
    "                    with torch.no_grad():\n",
    "                        DesignatedL1Max = mm.weight.size(0)*m.weight.size(1)*self.L1perUnit\n",
    "                        ClipL1NormLayer(DesignatedL1Max, mm, Counter)\n",
    "        return \n",
    "    \n",
    "    def DistributionRatio(self, points):\n",
    "### This is rho. I.e., after training, the estimator of the distribution ratio.\n",
    "        with torch.no_grad():\n",
    "            F = self(points)\n",
    "        return F/(1-F)\n",
    "\n",
    "    def InitPreprocess(self, Data, Parameters):\n",
    "### This can be run only ONCE to initialize the preprocess (shift and scaling) parameters\n",
    "### Takes as input the training Data and the training Parameters as Torch tensors.\n",
    "        if not hasattr(self, 'Scaling'):\n",
    "            print('Initializing Preprocesses Variables')\n",
    "            self.Scaling = Data.std(0)\n",
    "            self.Shift = Data.mean(0)\n",
    "            self.ParameterScaling = Parameters.std(0)  \n",
    "        else: print('Preprocess can be initialized only once. Parameters unchanged.')\n",
    "            \n",
    "    def Preprocess(self, Data, Parameters):\n",
    "### Returns scaled/shifted data and parameters\n",
    "### Takes as input Data and Parameters as Torch tensors.\n",
    "        if  not hasattr(self, 'Scaling'): print('Preprocess parameters are not initialized.')\n",
    "        Data = (Data - self.Shift)/self.Scaling\n",
    "        Parameters = Parameters/self.ParameterScaling\n",
    "        Ones = torch.ones([Parameters.size(0),1], dtype=Parameters.dtype)\n",
    "        if Parameters.is_cuda:\n",
    "            Ones = Ones.cuda()\n",
    "        Parameters = torch.cat([Ones, Parameters.reshape(Data.size(0), -1)], dim=1)\n",
    "        return Data, Parameters\n",
    "    \n",
    "    def Save(self, Name, Folder, csvFormat=False):\n",
    "### Saves the model in Folder/Name\n",
    "        FileName = Folder + Name + '.pth'\n",
    "        torch.save({'StateDict': self.state_dict(), \n",
    "                   'Scaling': self.Scaling,\n",
    "                   'Shift': self.Shift,\n",
    "                   'ParameterScaling': self.ParameterScaling}, \n",
    "                   FileName)\n",
    "        print('Model successfully saved.')\n",
    "        print('Path: %s'%str(FileName))\n",
    "        \n",
    "        if csvFormat:\n",
    "            modelparams = [w.detach().tolist() for w in self.parameters()]\n",
    "            np.savetxt(Folder + Name + ' (StateDict).csv', modelparams, '%s')\n",
    "            statistics = [self.Shift.detach().tolist(), self.Scaling.detach().tolist(),\n",
    "                         self.ParameterScaling.detach().tolist()]\n",
    "            np.savetxt(Folder + Name + ' (Statistics).csv', statistics, '%s')\n",
    "    \n",
    "    def Load(self, Name, Folder):\n",
    "### Loads the model from Folder/Name\n",
    "        FileName = Folder + Name + '.pth'\n",
    "        try:\n",
    "            IncompatibleKeys = self.load_state_dict(torch.load(FileName)['StateDict'])\n",
    "        except KeyError:\n",
    "            print('No state dictionary saved. Loading model failed.')\n",
    "            return \n",
    "        \n",
    "        if list(IncompatibleKeys)[0]:\n",
    "            print('Missing Keys: %s'%str(list(IncompatibleKeys)[0]))\n",
    "            print('Loading model failed. ')\n",
    "            return \n",
    "        \n",
    "        if list(IncompatibleKeys)[1]:\n",
    "            print('Unexpected Keys: %s'%str(list(IncompatibleKeys)[0]))\n",
    "            print('Loading model failed. ')\n",
    "            return \n",
    "        \n",
    "        self.Scaling = torch.load(FileName)['Scaling']\n",
    "        self.Shift = torch.load(FileName)['Shift']\n",
    "        self.ParameterScaling = torch.load(FileName)['ParameterScaling']\n",
    "        \n",
    "        print('Model successfully loaded.')\n",
    "        print('Path: %s'%str(FileName))\n",
    "        \n",
    "    def Report(self): ### is it possibe to check if the model is in double?\n",
    "        print('\\nModel Report:')\n",
    "        print('Preprocess Initialized: ' + str(hasattr(self, 'Shift')))\n",
    "        print('Architecture: ' + str(self.Architecture))\n",
    "        print('Loss Function: ' + 'Quadratic')\n",
    "        print('Activation: ' + str(self.ActivationFunction))\n",
    "        \n",
    "    def cuda(self):\n",
    "        nn.Module.cuda(self)\n",
    "        self.Shift = self.Shift.cuda()\n",
    "        self.Scaling = self.Scaling.cuda()\n",
    "        self.ParameterScaling = self.ParameterScaling.cuda()\n",
    "        \n",
    "    def cpu(self):\n",
    "        self.Shift = self.Shift.cpu()\n",
    "        self.Scaling = self.Scaling.cpu()\n",
    "        self.ParameterScaling = self.ParameterScaling.cpu()\n",
    "        return nn.Module.cpu(self)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurTrainingData():\n",
    "### Imports data for training. The Return() methods returns [self.Data, self.Labels, self.Weights, self.ParVal]\n",
    "### All values are in double precision\n",
    "### Inputs are the SM and BSM file paths and list of integers to chop the datasets if needed\n",
    "### Weights are normalized to have sum = 1 on the entire training sample\n",
    "    def __init__(self, SMfilepathlist, BSMfilepathlist, process, parameters, SMNLimits=\"NA\", BSMNLimits=\"NA\", verbose=True): \n",
    "        self.Process = process\n",
    "        self.Parameters = parameters\n",
    "        if verbose: print('Loading Data Files for Process: ' + str(self.Process) +', with new physics Parameters: ' + str(self.Parameters) ) \n",
    "        if len(self.Parameters)!= 1: print('Only 1D Implemented in Training !')   \n",
    "          \n",
    "####### Load BSM data (stored in self.BSMDataFiles)\n",
    "        if type(BSMfilepathlist) == list:\n",
    "            if all(isinstance(n, str) for n in BSMfilepathlist):\n",
    "                self.BSMDataFiles = [] \n",
    "                for path in BSMfilepathlist:\n",
    "                    temp =  DataFile(path, verbose=verbose)\n",
    "                    if((temp.Process == self.Process) and (set(list(temp.Parameters.flatten())) == set(self.Parameters)) and (sum(temp.Values.flatten()) != 0.) ):\n",
    "                        self.BSMDataFiles.append(temp)\n",
    "                    else: \n",
    "                        print('File not valid: ' + path)\n",
    "                        print('Parameters = ' + str(temp.Parameters) + ', Process = ' + str(temp.Process) \n",
    "                              +' and Values = ' + str(temp.Values.tolist()))\n",
    "                        print('should be = ' + str(self.Parameters) + ', = ' + str(self.Process) \n",
    "                              + ' and != ' + str(0.))\n",
    "                        raise ValueError\n",
    "                        self.BSMDataFiles.append(None) \n",
    "            else:\n",
    "                print('BSMfilepathlist input should be a list of strings !')\n",
    "                raise FileNotFoundError\n",
    "        else:\n",
    "            print('BSMfilepathlist input should be a list !')\n",
    "            raise FileNotFoundError\n",
    "                  \n",
    "###### Chop the BSM data sets (stored in BSMNDList, BSMDataList, BSMWeightsList, BSMParValList, BSMTargetList)\n",
    "        if type(BSMNLimits) == int:\n",
    "            BSMNLimits = [min(BSMNLimits, NF.ND) for NF in self.BSMDataFiles]\n",
    "        elif type(BSMNLimits) == list and all(isinstance(n, int) for n in BSMNLimits):\n",
    "            if len(BSMNLimits) != len(self.BSMDataFiles):\n",
    "                print(\"--> Please input %d integers to chop each SM file.\"%(\n",
    "                    len(self.BSMDataFiles)))\n",
    "                raise ValueError\n",
    "            elif sum([self.BSMDataFiles[i].ND >= BSMNLimits[i] for i in range(len(BSMNLimits))]\n",
    "                    ) != len(self.BSMDataFiles):\n",
    "                print(\"--> Some chop limit larger than available data in the corresponding file.\")\n",
    "                print(\"--> Lengths of the files: \"+str([file.ND for file in self.BSMDataFiles ]))\n",
    "                raise ValueError\n",
    "        else:\n",
    "            BSMNLimits =[file.ND for file in self.BSMDataFiles]   \n",
    "            \n",
    "        self.BSMNDList = BSMNLimits\n",
    "        #self.BSMNData = sum(self.BSMNDataList)\n",
    "        self.BSMDataList = [DF.Data[:N] for (DF, N) in zip(\n",
    "            self.BSMDataFiles, self.BSMNDList)]\n",
    "        self.BSMWeightsList = [DF.Weights[:N] for (DF, N) in zip(\n",
    "            self.BSMDataFiles, self.BSMNDList)] \n",
    "        self.BSMXSList = [DF.XS for DF in self.BSMDataFiles]\n",
    "        self.BSMParValList =  [torch.ones([N, len(self.Parameters)], dtype=torch.double)*DF.Values for (DF, N) in zip(self.BSMDataFiles, self.BSMNDList)]\n",
    "        self.BSMTargetList = [torch.ones(N, dtype=torch.double) for N in self.BSMNDList] \n",
    "        \n",
    "        \n",
    "####### Load SM data (stored in SMDataFiles)\n",
    "        if type(SMfilepathlist) == list:\n",
    "            if all(isinstance(n, str) for n in SMfilepathlist):\n",
    "                #self.SMFilePathList = SMfilepathlist\n",
    "                #self.SMNumFiles = len(self.SMFilePathList)\n",
    "                self.SMDataFiles = []\n",
    "                for path in SMfilepathlist:\n",
    "                    temp =  DataFile(path, verbose=verbose)\n",
    "                    if( (temp.Process == self.Process) and (temp.Parameters[0] == 'SM') and (sum(temp.Values.flatten()) == 0.) ):\n",
    "                        self.SMDataFiles.append(temp)\n",
    "                    else:\n",
    "                        print('File not valid: ' + path)\n",
    "                        print('Parameters = ' + str(temp.Parameters) + ', Process = ' + str(temp.Process) \n",
    "                              +' and Values = ' + str(temp.Values.tolist()))\n",
    "                        print('should be = ' + 'SM'+ ', = ' + str(self.Process) \n",
    "                              + ' and = ' + str(0.))\n",
    "                        self.SMDataFiles.append(None)                    \n",
    "            else:\n",
    "                print('SMfilepathlist input should be a list of strings !')\n",
    "                raise FileNotFoundError\n",
    "        else:\n",
    "            print('SMfilepathlist input should be a list !')\n",
    "            raise FileNotFoundError\n",
    "            \n",
    "####### Chop the SM data sets and join them in one (stored in SMND, SMData and SMWeights)\n",
    "        if type(SMNLimits) == int:\n",
    "            SMNLimits = [min(SMNLimits, DF.ND) for DF in self.SMDataFiles]\n",
    "        elif type(SMNLimits) == list and all(isinstance(n, int) for n in SMNLimits):\n",
    "            if len(SMNLimits) != len(self.SMDataFiles):\n",
    "                print(\"--> Please input %d integers to chop each SM file.\"%(\n",
    "                    len(self.SMDataFiles)))\n",
    "                raise ValueError\n",
    "            elif sum([self.SMDataFiles[i].ND >= SMNLimits[i] for i in range(len(SMNLimits))]\n",
    "                    ) != len(self.SMDataFiles):\n",
    "                print(\"--> Some chop limit larger than available data in the corresponding file.\")\n",
    "                print(\"--> Lengths of the files: \" + str([file.ND for file in self.SMDataFiles]))\n",
    "                raise ValueError\n",
    "        else:\n",
    "            SMNLimits = [file.ND for file in self.SMDataFiles]\n",
    "        self.SMND = sum(SMNLimits)\n",
    "        self.SMData = torch.cat(\n",
    "            [DF.Data[:N] for (DF, N) in zip(self.SMDataFiles, SMNLimits)]\n",
    "            , 0) \n",
    "        self.SMWeights = torch.cat(\n",
    "            [DF.Weights[:N] for (DF, N) in zip(self.SMDataFiles, SMNLimits)]\n",
    "            , 0)\n",
    "        self.SMXSList = [DF.XS for DF in self.SMDataFiles]\n",
    "        idx_random = torch.randperm(self.SMND)\n",
    "        self.SMData = self.SMData[idx_random, :]\n",
    "        self.SMWeights = self.SMWeights[idx_random]\n",
    "\n",
    "####### Break SM data in blocks to be paired with BSM data (stored in UsedSMNDList, UsedSMDataList, UsedSMWeightsList, UsedSMParValList, UsedSMTargetList)\n",
    "        BSMNRatioDataList = [torch.tensor(1., dtype=torch.double)*n/sum(self.BSMNDList\n",
    "                                                                       ) for n in self.BSMNDList]\n",
    "        self.UsedSMNDList = [int(self.SMND*BSMNRatioData) for BSMNRatioData in BSMNRatioDataList] \n",
    "        #self.UsedSMNData = sum(self.UsedSMNDataList)\n",
    "        #self.UsedSMData = self.SMData[: self.UsedSMND]\n",
    "        self.UsedSMDataList =  self.SMData[:sum(self.UsedSMNDList)].split(self.UsedSMNDList)\n",
    "        \n",
    "    ##### Reweighting is performed such that the SUM of the SM weights in each block equals the number of BSM data times the AVERAGE \n",
    "    ##### of the original weights. This equals the SM cross-section as obtained in the specific sample at hand, times NBSM\n",
    "        self.UsedSMWeightsList = self.SMWeights[:sum(self.UsedSMNDList)].split(self.UsedSMNDList)\n",
    "        self.UsedSMWeightsList = [ self.UsedSMWeightsList[i]*self.BSMNDList[i]/self.UsedSMNDList[i] for i in range(len(BSMNRatioDataList))]   \n",
    "        self.UsedSMParValList =  [torch.ones([N, len(self.Parameters)], dtype=torch.double)*DF.Values for (DF, N) in zip(self.BSMDataFiles, self.UsedSMNDList)]       \n",
    "        self.UsedSMTargetList = [torch.zeros(N, dtype=torch.double) for N in self.UsedSMNDList]\n",
    "\n",
    "####### Join SM with BSM data\n",
    "        self.Data = torch.cat(\n",
    "            [torch.cat([self.UsedSMDataList[i], self.BSMDataList[i]]\n",
    "                                  ) for i in range(len(self.BSMDataList))]\n",
    "            )\n",
    "        self.Weights = torch.cat(\n",
    "            [torch.cat([self.UsedSMWeightsList[i], self.BSMWeightsList[i]]\n",
    "                                  ) for i in range(len(self.BSMWeightsList))]\n",
    "            )\n",
    "        self.Labels = torch.cat(\n",
    "            [torch.cat([self.UsedSMTargetList[i], self.BSMTargetList[i]]\n",
    "                                  ) for i in range(len(self.BSMTargetList))]\n",
    "            )\n",
    "        self.ParVal = torch.cat(\n",
    "            [torch.cat([self.UsedSMParValList[i], self.BSMParValList[i]]\n",
    "                                  ) for i in range(len(self.BSMParValList))]\n",
    "            )\n",
    "        \n",
    "####### Final reweighting\n",
    "        s = self.Weights.sum()\n",
    "        self.Weights = self.Weights.div(s)\n",
    "\n",
    "####### If verbose, display report\n",
    "        if verbose: self.Report()\n",
    "        \n",
    "####### Return Tranining Data\n",
    "    def ReturnData(self):\n",
    "        return [self.Data, self.Labels, self.Weights, self.ParVal]\n",
    "            \n",
    "    def Report(self):\n",
    "        #from tabulate import tabulate\n",
    "        print('\\nLoaded SM Files:')\n",
    "        print(tabulate({str(self.Parameters): [ file.Values for file in self.SMDataFiles ], \n",
    "                        \"#Data\":[ file.ND for file in self.SMDataFiles ], \n",
    "                        \"XS[pb](avg.w)\":[ file.XS for file in self.SMDataFiles ]}, headers=\"keys\"))\n",
    "        print('\\nLoaded BSM Files:')\n",
    "        print(tabulate({str(self.Parameters): [ file.Values for file in self.BSMDataFiles ], \n",
    "                        \"#Data\":[ file.ND for file in self.BSMDataFiles ], \n",
    "                        \"XS[pb](avg.w)\":[ file.XS for file in self.BSMDataFiles ]}, headers=\"keys\"))\n",
    "        print('\\nPaired BSM/SM Datasets:\\n')\n",
    "        ### Check should be nearly equal to #EV.BSM. It is computed with the weights BEFORE final reweighting\n",
    "        print(tabulate({str(self.Parameters): [ file.Values for file in self.BSMDataFiles ], \"#Ev.BSM\": self.BSMNDList\n",
    "                        , \"#Ev.SM\": self.UsedSMNDList,\n",
    "                        \"Check\": [(self.UsedSMWeightsList[i].sum())/(self.SMWeights.mean()) for i in range(len(self.BSMDataFiles))]\n",
    "                       }, headers=\"keys\"))    \n",
    "        \n",
    "####### Convert Angles\n",
    "    def CurateAngles(self, AnglePos):\n",
    "        Angles = self.Data[:, AnglePos]\n",
    "        CuratedAngles = torch.cat([torch.sin(Angles), torch.cos(Angles)], dim=1)\n",
    "        OtherPos = list(set(range(self.Data.size(1)))-set(AnglePos))\n",
    "        self.Data = torch.cat([self.Data[:, OtherPos], CuratedAngles], dim=1)\n",
    "        print('####\\nAnlges at position %s have been converted to Sin and Cos and put at the last columns of the Data.'%(AnglePos))\n",
    "        print('####')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data Files for Process: W+Z, with new physics Parameters: ['GW[TeV**-2]', 'Gphi[TeV**-2]']\n",
      "Only 1D Implemented in Training !\n",
      "\n",
      "Reading file .../data3/WZ_new_project/h5/Ideal_Data/ChP_pt300_gphi2e-1_gw1e-1.h5\n",
      "##### File Info:\n",
      "{Gphi[TeV**-2], GW[TeV**-2]} = {0.2, 0.1}[TeV**-2] data, Ideal Events. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ}, weight}.\n",
      "Converted from /data3/WZ_new_project/dat/Ideal_Events/ChP_pt300_gphi2e-1_gw1e-1.dat.gz\n",
      "Charge = 1 --- Process = W+Z\n",
      "#####\n",
      "\n",
      "Reading file .../data3/WZ_new_project/h5/Ideal_Data/ChP_pt300_gphi5e-1_gw2e-1.h5\n",
      "##### File Info:\n",
      "{Gphi[TeV**-2], GW[TeV**-2]} = {0.5, 0.2}[TeV**-2] data, Ideal Events. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ}, weight}.\n",
      "Converted from /data3/WZ_new_project/dat/Ideal_Events/ChP_pt300_gphi5e-1_gw2e-1.dat.gz\n",
      "Charge = 1 --- Process = W+Z\n",
      "#####\n",
      "\n",
      "Reading file .../data3/WZ_new_project/h5/Ideal_Data/ChP_pt300_gphim2e-1_gw1e-1.h5\n",
      "##### File Info:\n",
      "{Gphi[TeV**-2], GW[TeV**-2]} = {-0.2, 0.1}[TeV**-2] data, Ideal Events. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ}, weight}.\n",
      "Converted from /data3/WZ_new_project/dat/Ideal_Events/ChP_pt300_gphim2e-1_gw1e-1.dat.gz\n",
      "Charge = 1 --- Process = W+Z\n",
      "#####\n",
      "\n",
      "Reading file .../data3/WZ_new_project/h5/Ideal_Data/ChP_pt300_gphim5e-1_gw2e-1.h5\n",
      "##### File Info:\n",
      "{Gphi[TeV**-2], GW[TeV**-2]} = {-0.5, 0.2}[TeV**-2] data, Ideal Events. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ}, weight}.\n",
      "Converted from /data3/WZ_new_project/dat/Ideal_Events/ChP_pt300_gphim5e-1_gw2e-1.dat.gz\n",
      "Charge = 1 --- Process = W+Z\n",
      "#####\n",
      "\n",
      "Reading file .../data3/WZ_new_project/h5/Ideal_Data/ChP_pt300_gw1e-1.h5\n",
      "##### File Info:\n",
      "{Gphi[TeV**-2], GW[TeV**-2]} = {0., 0.1}[TeV**-2] data, Ideal Events. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ}, weight}.\n",
      "Converted from /data3/WZ_new_project/dat/Ideal_Events/ChP_pt300_gw1e-1.dat.gz\n",
      "Charge = 1 --- Process = W+Z\n",
      "#####\n",
      "\n",
      "Reading file .../data3/WZ_new_project/h5/Ideal_Data/ChP_pt300_gw2e-1.h5\n",
      "##### File Info:\n",
      "{Gphi[TeV**-2], GW[TeV**-2]} = {0., 0.2}[TeV**-2] data, Ideal Events. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ}, weight}.\n",
      "Converted from /data3/WZ_new_project/dat/Ideal_Events/ChP_pt300_gw2e-1.dat.gz\n",
      "Charge = 1 --- Process = W+Z\n",
      "#####\n",
      "\n",
      "Reading file .../data3/WZ_new_project/h5/Ideal_Data/ChP_pt300_sm_1.h5\n",
      "##### File Info:\n",
      "SM = {0., 0.}[TeV**-2] data, Ideal Events. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ}, weight}.\n",
      "Converted from /data3/WZ_new_project/dat/Ideal_Events/ChP_pt300_sm_1.dat.gz\n",
      "Charge = 1 --- Process = W+Z\n",
      "#####\n",
      "\n",
      "Loaded SM Files:\n",
      "['GW[TeV**-2]', 'Gphi[TeV**-2]']           #Data    XS[pb](avg.w)\n",
      "---------------------------------------  -------  ---------------\n",
      "tensor([[0., 0.]], dtype=torch.float64)  3000000         0.741835\n",
      "\n",
      "Loaded BSM Files:\n",
      "['GW[TeV**-2]', 'Gphi[TeV**-2]']                     #Data    XS[pb](avg.w)\n",
      "-------------------------------------------------  -------  ---------------\n",
      "tensor([[0.2000, 0.1000]], dtype=torch.float64)     500000          2.45948\n",
      "tensor([[0.5000, 0.2000]], dtype=torch.float64)     500000          8.4987\n",
      "tensor([[-0.2000,  0.1000]], dtype=torch.float64)   500000          1.22171\n",
      "tensor([[-0.5000,  0.2000]], dtype=torch.float64)   500000          5.40484\n",
      "tensor([[0.0000, 0.1000]], dtype=torch.float64)     500000          1.03439\n",
      "tensor([[0.0000, 0.2000]], dtype=torch.float64)     500000          1.91103\n",
      "\n",
      "Paired BSM/SM Datasets:\n",
      "\n",
      "['GW[TeV**-2]', 'Gphi[TeV**-2]']                     #Ev.BSM    #Ev.SM    Check\n",
      "-------------------------------------------------  ---------  --------  -------\n",
      "tensor([[0.2000, 0.1000]], dtype=torch.float64)       500000    500000   500000\n",
      "tensor([[0.5000, 0.2000]], dtype=torch.float64)       500000    500000   500000\n",
      "tensor([[-0.2000,  0.1000]], dtype=torch.float64)     500000    500000   500000\n",
      "tensor([[-0.5000,  0.2000]], dtype=torch.float64)     500000    500000   500000\n",
      "tensor([[0.0000, 0.1000]], dtype=torch.float64)       500000    500000   500000\n",
      "tensor([[0.0000, 0.2000]], dtype=torch.float64)       500000    500000   500000\n"
     ]
    }
   ],
   "source": [
    "DataFolder = '/data3/WZ_new_project/h5/Ideal_Data'\n",
    "\n",
    "td = OurTrainingData([DataFolder + '/ChP_pt300_sm_1.h5',],\n",
    "                    [DataFolder + '/ChP_pt300_gphi2e-1_gw1e-1.h5',\n",
    "                     DataFolder + '/ChP_pt300_gphi5e-1_gw2e-1.h5',\n",
    "                     DataFolder + '/ChP_pt300_gphim2e-1_gw1e-1.h5',\n",
    "                     DataFolder + '/ChP_pt300_gphim5e-1_gw2e-1.h5',\n",
    "                     DataFolder + '/ChP_pt300_gw1e-1.h5',\n",
    "                     DataFolder + '/ChP_pt300_gw2e-1.h5'],\n",
    "                     process = 'W+Z', parameters =['GW[TeV**-2]', 'Gphi[TeV**-2]'], \n",
    "                     SMNLimits=int(3e6),\n",
    "                     BSMNLimits=int(5e5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####\n",
      "Anlges at position [3, 5] have been converted to Sin and Cos and put at the last columns of the Data.\n",
      "####\n",
      "Initializing Preprocesses Variables\n",
      "Training epoch 10 (took 24.89 sec, time left 6:16:43.728622 sec) loss 0.14230862\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/2DQuadraticClassifier/TrainedModels/ChPgwgphi, (500k, CD), 10000 Epochs10 epoch.pth\n",
      "Training epoch 100 (took 222.95 sec, time left 6:44:51.141399 sec) loss 0.11960425\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/2DQuadraticClassifier/TrainedModels/ChPgwgphi, (500k, CD), 10000 Epochs100 epoch.pth\n",
      "Training epoch 500 (took 999.41 sec, time left 6:34:08.012994 sec) loss 0.11381173\n",
      "Model successfully saved.\n",
      "Path: /home/chen/Documents/2DQuadraticClassifier/TrainedModels/ChPgwgphi, (500k, CD), 10000 Epochs500 epoch.pth\n"
     ]
    }
   ],
   "source": [
    "NumEpochs = int(1e4)\n",
    "\n",
    "td.Data = td.Data[:, :7]\n",
    "td.CurateAngles([3, 5])\n",
    "\n",
    "Data, ParVal, Labels, Weights = td.Data, td.ParVal, td.Labels, td.Weights\n",
    "Data, ParVal, Labels, Weights = Data.float(), ParVal.float(), Labels.float(), Weights.float()\n",
    "\n",
    "MD = OurCDModel(NumberOfParameters=2, AR=[9,32,32,32,1])\n",
    "MD.InitPreprocess(Data, ParVal)\n",
    "\n",
    "OT = OurTrainer(NumEpochs = NumEpochs)\n",
    "OT.SetSaveAfterEpochs([10,100,500]+list(range(1000, 11000, 1000)))\n",
    "\n",
    "OT.Train(MD, Data = Data, Parameters = ParVal, Labels=Labels, Weights= Weights, bs = 100000,\n",
    "        Name = 'ChPgwgphi, (500k, CD), %d Epochs'%NumEpochs, Folder = os.getcwd()+'/TrainedModels/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
