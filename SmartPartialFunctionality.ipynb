{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== Random Seed: 96 ===========\n"
     ]
    }
   ],
   "source": [
    "from OurTrainingTools2D import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OurCDModel(nn.Module):\n",
    "### Defines the  model with parametrized discriminant. Only quadratic dependence on a single parameter is implemented.\n",
    "### Input is the architecture (list of integers, the last one being equal to 1) and the activation type ('ReLU' or 'Sigmoid')\n",
    "    def __init__(self, NumberOfParameters, AR = [1, 3, 3, 1] , AF = 'ReLU'):               \n",
    "        super(OurCDModel, self).__init__() \n",
    "        ValidActivationFunctions = {'ReLU': torch.relu, 'Sigmoid': torch.sigmoid}\n",
    "        try:\n",
    "            self.ActivationFunction = ValidActivationFunctions[AF]\n",
    "        except KeyError:\n",
    "            print('The activation function specified is not valid. Allowed activations are %s.'\n",
    "                 %str(list(ValidActivationFunctions.keys())))\n",
    "            print('Will use ReLU.')\n",
    "            self.ActivationFunction = torch.relu            \n",
    "        if type(AR) == list:\n",
    "            if( ( all(isinstance(n, int) for n in AR)) and ( AR[-1] == 1) ):\n",
    "                self.Architecture = AR\n",
    "            else:\n",
    "                print('Architecture should be a list of integers, the last one should be 1.')\n",
    "                raise ValueError             \n",
    "        else:\n",
    "            print('Architecture should be a list !')\n",
    "            raise ValueError\n",
    "        self.DefineLayers(NumberOfParameters)\n",
    "\n",
    "### Define Layers\n",
    "    def DefineLayers(self, NumberOfParameters):\n",
    "        print('====== Defining layers for %d parameters. ======'%(NumberOfParameters))\n",
    "        self.NumberOfParameters = NumberOfParameters\n",
    "        self.NumberOfNetworks = int((2+self.NumberOfParameters)*(1+self.NumberOfParameters)/2)-1\n",
    "        LinearLayers = [([nn.Linear(self.Architecture[i], self.Architecture[i+1]) \\\n",
    "                                  for i in range(len(self.Architecture)-1)])\\\n",
    "                        for n in range(self.NumberOfNetworks)]\n",
    "        LinearLayers = [Layer for SubLayerList in LinearLayers for Layer in SubLayerList]\n",
    "        self.LinearLayers = nn.ModuleList(LinearLayers)\n",
    "    \n",
    "    def Forward(self, Data, Parameters):\n",
    "### Forward Function. Performs Preprocessing, returns F = rho/(1+rho) in [0,1], where rho is quadratically parametrized.\n",
    "        # Checking that data has the right input dimension\n",
    "        InputDimension = self.Architecture[0]\n",
    "        if Data.size(1) != InputDimension:\n",
    "            print('Dimensions of the data and the network input mismatch: data: %d, model: %d'\n",
    "                  %(Data.size(1), InputDimension))\n",
    "            raise ValueError\n",
    "\n",
    "        # Checking that preprocess has been initialised\n",
    "        if not hasattr(self, 'Shift'):\n",
    "            print('Please initialize preprocess parameters!')\n",
    "            raise ValueError\n",
    "        if not hasattr(self, 'IsParamRedundant'):\n",
    "            print('Please make sure that you have checked for Parameter redundancy.')\n",
    "            raise ValueError\n",
    "            \n",
    "        if self.IsParamRedundant:\n",
    "            Parameters = self.Parameters_Short\n",
    "            \n",
    "        with torch.no_grad(): \n",
    "            Data, Parameters = self.Preprocess(Data, Parameters)  \n",
    "        \n",
    "        print(Parameters)\n",
    "        \n",
    "        NumberOfLayers, NumberOfEvents = len(self.Architecture)-1, Data.size(0)\n",
    "        EntryIterator, NetworkIterator = 0, -1\n",
    "        MatrixLT = torch.zeros([NumberOfEvents, (self.NumberOfParameters+1)**2], dtype=Data.dtype)\n",
    "        \n",
    "        if Data.is_cuda:\n",
    "            MatrixLT = OurCudaTensor(MatrixLT)\n",
    "        \n",
    "        for i in range(self.NumberOfParameters+1):\n",
    "            EntryIterator += i\n",
    "            DiagonalEntry = True\n",
    "            for j in range(self.NumberOfParameters+1-i):\n",
    "                if NetworkIterator == -1:\n",
    "                    MatrixLT[:, EntryIterator] = torch.ones(NumberOfEvents)\n",
    "                    #print('Entry: %d, Layer: ones, DiagonalEntry: %s'%(EntryIterator,\n",
    "                    #                                                str(DiagonalEntry)))\n",
    "                else:\n",
    "                    x = Data\n",
    "                    for Layer in self.LinearLayers[NumberOfLayers*NetworkIterator:\\\n",
    "                                                  NumberOfLayers*(NetworkIterator+1)-1]:\n",
    "                        x = self.ActivationFunction(Layer(x))\n",
    "                    x = self.LinearLayers[NumberOfLayers*(NetworkIterator+1)-1](x).squeeze()\n",
    "                    #MatrixLT[:, EntryIterator] = torch.exp(x) if DiagonalEntry else x\n",
    "                    MatrixLT[:, EntryIterator] = x\n",
    "                    #print('Entry: %d, Layer: %d, DiagonalEntry: %s'%(EntryIterator, NetworkIterator, \n",
    "                    #                                                str(DiagonalEntry)))\n",
    "                EntryIterator += 1\n",
    "                NetworkIterator += 1\n",
    "                DiagonalEntry = False\n",
    "        #print('MatrixLT: '+str(MatrixLT.is_cuda))\n",
    "        #print('Parameters: '+str(Parameters.is_cuda))\n",
    "\n",
    "        MatrixLT = MatrixLT.reshape([-1, self.NumberOfParameters+1, self.NumberOfParameters+1]) \n",
    "        MatrixLTP = MatrixLT.matmul(Parameters.reshape([NumberOfEvents, self.NumberOfParameters+1, 1]))\n",
    "        rho = MatrixLTP.permute([0, 2, 1]).matmul(MatrixLTP).squeeze()\n",
    "        \n",
    "        return (rho.div(1.+rho)).view(-1, 1)\n",
    "    \n",
    "    def GetL1Bound(self, L1perUnit):\n",
    "        self.L1perUnit = L1perUnit\n",
    "    \n",
    "    def ClipL1Norm(self):\n",
    "### Clip the weights      \n",
    "        def ClipL1NormLayer(DesignatedL1Max, Layer, Counter):\n",
    "            if Counter == 1:\n",
    "                ### this avoids clipping the first layer\n",
    "                return\n",
    "            L1 = Layer.weight.abs().sum()\n",
    "            Layer.weight.masked_scatter_(L1 > DesignatedL1Max, \n",
    "                                        Layer.weight*(DesignatedL1Max/L1))\n",
    "            return\n",
    "        \n",
    "        Counter = 0\n",
    "        for m in self.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                Counter += 1\n",
    "                with torch.no_grad():\n",
    "                    DesignatedL1Max = m.weight.size(0)*m.weight.size(1)*self.L1perUnit\n",
    "                    ClipL1NormLayer(DesignatedL1Max, m, Counter)\n",
    "            else:\n",
    "                for mm in m:\n",
    "                    Counter +=1\n",
    "                    with torch.no_grad():\n",
    "                        DesignatedL1Max = mm.weight.size(0)*m.weight.size(1)*self.L1perUnit\n",
    "                        ClipL1NormLayer(DesignatedL1Max, mm, Counter)\n",
    "        return \n",
    "    \n",
    "    def DistributionRatio(self, points):\n",
    "### This is rho. I.e., after training, the estimator of the distribution ratio.\n",
    "        with torch.no_grad():\n",
    "            F = self(points)\n",
    "        return F/(1-F)\n",
    "    \n",
    "    def checkRedundancy(self, Parameters):\n",
    "### This is written specifically for 2D networks. It will check if any columns of the parameters are redundant \n",
    "### (i.e., full of zeros), and adjust the number of networks as well as the parameters scalings.\n",
    "### Of course the Forward function will also check the self.IsParamRedundant attribute to see which Parameters\n",
    "### to use.\n",
    "        print('====== Checking parameter redundancy. ======')\n",
    "        \n",
    "        Param_idx = torch.arange(Parameters.size(1))\n",
    "        zero_idx  = (torch.nonzero(Parameters[0] == Parameters[1]+Parameters[0])) # possible zero columns\n",
    "        zero_mask = torch.tensor([len(torch.nonzero(Parameters[:, zero_idx.squeeze()] !=0)\n",
    "                                     )!=0 if idx in zero_idx else True for idx in Param_idx])\n",
    "        self.IsParamRedundant = (sum(zero_mask) != Parameters.size(1))\n",
    "        print('====== IsParamRedundant: ' + str(self.IsParamRedundant))\n",
    "        if self.IsParamRedundant:\n",
    "            good_parameters = torch.nonzero(zero_mask)\n",
    "            print(good_parameters)\n",
    "            print('====== Effective parameters: ' + str(list(good_parameters)))\n",
    "            self.Parameters_Short = Parameters[:, good_parameters]\n",
    "            if Parameters.is_cuda():\n",
    "                self.Parameters_Short = self.Parameters_Short.cuda()\n",
    "            self.DefineLayers(len(good_parameters))\n",
    "            \n",
    "\n",
    "    def InitPreprocess(self, Data, Parameters):\n",
    "### This can be run only ONCE to initialize the preprocess (shift and scaling) parameters\n",
    "### Takes as input the training Data and the training Parameters as Torch tensors.\n",
    "        \n",
    "        # check redunancy\n",
    "        self.checkRedundancy(Parameters)\n",
    "        \n",
    "        if not hasattr(self, 'Scaling'):\n",
    "            print('Initializing Preprocesses Variables')\n",
    "            self.Scaling = Data.std(0)\n",
    "            self.Shift = Data.mean(0)\n",
    "            self.ParameterScaling = self.Parameters_Short.std(0\n",
    "                                ) if self.IsParamRedundant else Parameters.std(0)             \n",
    "        else: print('Preprocess can be initialized only once. Parameters unchanged.')\n",
    "            \n",
    "    def Preprocess(self, Data, Parameters):\n",
    "### Returns scaled/shifted data and parameters\n",
    "### Takes as input Data and Parameters as Torch tensors.\n",
    "        if  not hasattr(self, 'Scaling'): print('Preprocess parameters are not initialized.')\n",
    "        Data = (Data - self.Shift)/self.Scaling\n",
    "        Parameters = Parameters/self.ParameterScaling\n",
    "        Ones = torch.ones([Parameters.size(0),1], dtype=Parameters.dtype)\n",
    "        if Parameters.is_cuda:\n",
    "            Ones = Ones.cuda()\n",
    "        Parameters = torch.cat([Ones, Parameters.reshape(Data.size(0), -1)], dim=1)\n",
    "        return Data, Parameters\n",
    "    \n",
    "    def Save(self, Name, Folder, csvFormat=False):\n",
    "### Saves the model in Folder/Name\n",
    "        FileName = Folder + Name + '.pth'\n",
    "        torch.save({'StateDict': self.state_dict(), \n",
    "                   'Scaling': self.Scaling,\n",
    "                   'Shift': self.Shift,\n",
    "                   'ParameterScaling': self.ParameterScaling}, \n",
    "                   FileName)\n",
    "        print('Model successfully saved.')\n",
    "        print('Path: %s'%str(FileName))\n",
    "        \n",
    "        if csvFormat:\n",
    "            modelparams = [w.detach().tolist() for w in self.parameters()]\n",
    "            np.savetxt(Folder + Name + ' (StateDict).csv', modelparams, '%s')\n",
    "            statistics = [self.Shift.detach().tolist(), self.Scaling.detach().tolist(),\n",
    "                         self.ParameterScaling.detach().tolist()]\n",
    "            np.savetxt(Folder + Name + ' (Statistics).csv', statistics, '%s')\n",
    "    \n",
    "    def Load(self, Name, Folder):\n",
    "### Loads the model from Folder/Name\n",
    "        FileName = Folder + Name + '.pth'\n",
    "        try:\n",
    "            IncompatibleKeys = self.load_state_dict(torch.load(FileName)['StateDict'])\n",
    "        except KeyError:\n",
    "            print('No state dictionary saved. Loading model failed.')\n",
    "            return \n",
    "        \n",
    "        if list(IncompatibleKeys)[0]:\n",
    "            print('Missing Keys: %s'%str(list(IncompatibleKeys)[0]))\n",
    "            print('Loading model failed. ')\n",
    "            return \n",
    "        \n",
    "        if list(IncompatibleKeys)[1]:\n",
    "            print('Unexpected Keys: %s'%str(list(IncompatibleKeys)[0]))\n",
    "            print('Loading model failed. ')\n",
    "            return \n",
    "        \n",
    "        self.Scaling = torch.load(FileName)['Scaling']\n",
    "        self.Shift = torch.load(FileName)['Shift']\n",
    "        self.ParameterScaling = torch.load(FileName)['ParameterScaling']\n",
    "        \n",
    "        print('Model successfully loaded.')\n",
    "        print('Path: %s'%str(FileName))\n",
    "        \n",
    "    def Report(self): ### is it possibe to check if the model is in double?\n",
    "        print('\\nModel Report:')\n",
    "        print('Preprocess Initialized: ' + str(hasattr(self, 'Shift')))\n",
    "        print('Architecture: ' + str(self.Architecture))\n",
    "        print('Loss Function: ' + 'Quadratic')\n",
    "        print('Activation: ' + str(self.ActivationFunction))\n",
    "        \n",
    "    def cuda(self):\n",
    "        nn.Module.cuda(self)\n",
    "        self.Shift = self.Shift.cuda()\n",
    "        self.Scaling = self.Scaling.cuda()\n",
    "        self.ParameterScaling = self.ParameterScaling.cuda()\n",
    "        \n",
    "    def cpu(self):\n",
    "        self.Shift = self.Shift.cpu()\n",
    "        self.Scaling = self.Scaling.cpu()\n",
    "        self.ParameterScaling = self.ParameterScaling.cpu()\n",
    "        return nn.Module.cpu(self)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Data Files for Process: W+Z, with new physics Parameters: ['Gphi[TeV**-2]', 'GW[TeV**-2]']\n",
      "\n",
      "Reading file .../data3/WZ_new_project/h5/Ideal_Data/ChP_pt300_gphim5e-1.h5\n",
      "##### File Info:\n",
      "{Gphi[TeV**-2], GW[TeV**-2]} = {-0.5, 0.}[TeV**-2] data, Ideal Events. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ}, weight}.\n",
      "Converted from /data3/WZ_new_project/dat/Ideal_Events/ChP_pt300_gphim5e-1.dat.gz\n",
      "Charge = 1 --- Process = W+Z\n",
      "#####\n",
      "\n",
      "Reading file .../data3/WZ_new_project/h5/Ideal_Data/ChP_pt300_gphim2e-1.h5\n",
      "##### File Info:\n",
      "{Gphi[TeV**-2], GW[TeV**-2]} = {-0.2, 0.}[TeV**-2] data, Ideal Events. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ}, weight}.\n",
      "Converted from /data3/WZ_new_project/dat/Ideal_Events/ChP_pt300_gphim2e-1.dat.gz\n",
      "Charge = 1 --- Process = W+Z\n",
      "#####\n",
      "\n",
      "Reading file .../data3/WZ_new_project/h5/Ideal_Data/ChP_pt300_gphim5e-2.h5\n",
      "##### File Info:\n",
      "{Gphi[TeV**-2], GW[TeV**-2]} = {-0.05, 0.}[TeV**-2] data, Ideal Events. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ}, weight}.\n",
      "Converted from /data3/WZ_new_project/dat/Ideal_Events/ChP_pt300_gphim5e-2.dat.gz\n",
      "Charge = 1 --- Process = W+Z\n",
      "#####\n",
      "\n",
      "Reading file .../data3/WZ_new_project/h5/Ideal_Data/ChP_pt300_gphi5e-1.h5\n",
      "##### File Info:\n",
      "{Gphi[TeV**-2], GW[TeV**-2]} = {0.5, 0.}[TeV**-2] data, Ideal Events. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ}, weight}.\n",
      "Converted from /data3/WZ_new_project/dat/Ideal_Events/ChP_pt300_gphi5e-1.dat.gz\n",
      "Charge = 1 --- Process = W+Z\n",
      "#####\n",
      "\n",
      "Reading file .../data3/WZ_new_project/h5/Ideal_Data/ChP_pt300_gphi2e-1.h5\n",
      "##### File Info:\n",
      "{Gphi[TeV**-2], GW[TeV**-2]} = {0.2, 0.}[TeV**-2] data, Ideal Events. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ}, weight}.\n",
      "Converted from /data3/WZ_new_project/dat/Ideal_Events/ChP_pt300_gphi2e-1.dat.gz\n",
      "Charge = 1 --- Process = W+Z\n",
      "#####\n",
      "\n",
      "Reading file .../data3/WZ_new_project/h5/Ideal_Data/ChP_pt300_gphi5e-2.h5\n",
      "##### File Info:\n",
      "{Gphi[TeV**-2], GW[TeV**-2]} = {0.05, 0.}[TeV**-2] data, Ideal Events. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ}, weight}.\n",
      "Converted from /data3/WZ_new_project/dat/Ideal_Events/ChP_pt300_gphi5e-2.dat.gz\n",
      "Charge = 1 --- Process = W+Z\n",
      "#####\n",
      "\n",
      "Reading file .../data3/WZ_new_project/h5/Ideal_Data/ChP_pt300_sm_1.h5\n",
      "##### File Info:\n",
      "SM = {0., 0.}[TeV**-2] data, Ideal Events. \n",
      "Event format: {{s, θ, θZ, ϕZ, θWrec, ϕWrec, PtZ}, weight}.\n",
      "Converted from /data3/WZ_new_project/dat/Ideal_Events/ChP_pt300_sm_1.dat.gz\n",
      "Charge = 1 --- Process = W+Z\n",
      "#####\n",
      "\n",
      "Loaded SM Files:\n",
      "['Gphi[TeV**-2]', 'GW[TeV**-2]']           #Data    XS[pb](avg.w)\n",
      "---------------------------------------  -------  ---------------\n",
      "tensor([[0., 0.]], dtype=torch.float64)  3000000         0.741835\n",
      "\n",
      "Loaded BSM Files:\n",
      "['Gphi[TeV**-2]', 'GW[TeV**-2]']                     #Data    XS[pb](avg.w)\n",
      "-------------------------------------------------  -------  ---------------\n",
      "tensor([[-0.5000,  0.0000]], dtype=torch.float64)   500000         4.23431\n",
      "tensor([[-0.2000,  0.0000]], dtype=torch.float64)   500000         0.929192\n",
      "tensor([[-0.0500,  0.0000]], dtype=torch.float64)   500000         0.637632\n",
      "tensor([[0.5000, 0.0000]], dtype=torch.float64)     500000         7.32829\n",
      "tensor([[0.2000, 0.0000]], dtype=torch.float64)     500000         2.16749\n",
      "tensor([[0.0500, 0.0000]], dtype=torch.float64)     500000         0.947113\n",
      "\n",
      "Paired BSM/SM Datasets:\n",
      "\n",
      "['Gphi[TeV**-2]', 'GW[TeV**-2]']                     #Ev.BSM    #Ev.SM    Check\n",
      "-------------------------------------------------  ---------  --------  -------\n",
      "tensor([[-0.5000,  0.0000]], dtype=torch.float64)     500000    500000   500000\n",
      "tensor([[-0.2000,  0.0000]], dtype=torch.float64)     500000    500000   500000\n",
      "tensor([[-0.0500,  0.0000]], dtype=torch.float64)     500000    500000   500000\n",
      "tensor([[0.5000, 0.0000]], dtype=torch.float64)       500000    500000   500000\n",
      "tensor([[0.2000, 0.0000]], dtype=torch.float64)       500000    500000   500000\n",
      "tensor([[0.0500, 0.0000]], dtype=torch.float64)       500000    500000   500000\n",
      "####\n",
      "Anlges at position [3, 5] have been converted to Sin and Cos and put at the last columns of the Data.\n",
      "####\n"
     ]
    }
   ],
   "source": [
    "DataFolder = '/data3/WZ_new_project/h5/Ideal_Data'\n",
    "\n",
    "td = OurTrainingData([DataFolder + '/ChP_pt300_sm_1.h5',\n",
    "                     ],\n",
    "                     [DataFolder + '/ChP_pt300_gphim5e-1.h5',\n",
    "                     DataFolder + '/ChP_pt300_gphim2e-1.h5',\n",
    "                     DataFolder + '/ChP_pt300_gphim5e-2.h5',\n",
    "                     DataFolder + '/ChP_pt300_gphi5e-1.h5',\n",
    "                     DataFolder + '/ChP_pt300_gphi2e-1.h5',\n",
    "                     DataFolder + '/ChP_pt300_gphi5e-2.h5'],\n",
    "                     process = 'W+Z', parameters =['Gphi[TeV**-2]', 'GW[TeV**-2]'], \n",
    "                     SMNLimits=int(3e6),\n",
    "                     BSMNLimits=int(5e5))\n",
    "\n",
    "NumEpochs = int(1e4)\n",
    "\n",
    "td.Data = td.Data[:, :7]\n",
    "td.CurateAngles([3, 5])\n",
    "\n",
    "Data, ParVal, Labels, Weights = td.Data, td.ParVal, td.Labels, td.Weights\n",
    "Data, ParVal, Labels, Weights = Data.float(), ParVal.float(), Labels.float(), Weights.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mute_idx = (torch.nonzero(ParVal[0] == ParVal[1]+ParVal[0]))\n",
    "\n",
    "mute_idx = (torch.nonzero(ParVal[0] == ParVal[1]))\n",
    "\n",
    "mute_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#mute_candidate = torch.tensor([[1,],[0,]])\n",
    "#mute_candidate = torch.tensor([[1,]])\n",
    "\n",
    "for col_idx in mute_idx:\n",
    "    print(len(torch.nonzero(ParVal[:, col_idx]!=0))==0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5000,  0.0000],\n",
       "        [-0.5000,  0.0000],\n",
       "        [-0.5000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.0500,  0.0000],\n",
       "        [ 0.0500,  0.0000],\n",
       "        [ 0.0500,  0.0000]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_idx = (torch.nonzero(ParVal[0] == ParVal[1]+ParVal[0])) # possible zero columns\n",
    "zero_idx = torch.tensor([len(torch.nonzero(ParVal[:, col_idx]==0))==0 for col_idx in zero_idx]) # check for zeros\n",
    "ParVal_Short = ParVal[:, torch.nonzero(zero_idx).squeeze()]\n",
    "ParVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[      0,       0],\n",
       "         [      1,       0],\n",
       "         [      2,       0],\n",
       "         ...,\n",
       "         [5999997,       0],\n",
       "         [5999998,       0],\n",
       "         [5999999,       0]])]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_idx = (torch.nonzero(ParVal[0] == ParVal[1]+ParVal[0])) # possible zero columns\n",
    "#zero_idx = torch.tensor([len(torch.nonzero(ParVal[:, col_idx]==0))==0 for col_idx in zero_idx]) # check for zeros\n",
    "[(torch.nonzero(ParVal[:, col_idx]==0)) for col_idx in zero_idx]\n",
    "#zero_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_idx = (torch.nonzero(ParVal[0] == ParVal[1]+ParVal[0])) # possible zero columns\n",
    "zero_mask = torch.tensor([len(torch.nonzero(ParVal[:, zero_idx.squeeze()] !=0)\n",
    "                             )!=0 if idx in zero_idx else True for idx in ParVal_idx])\n",
    "ParVal_Short = ParVal[:, torch.nonzero(zero_mask).squeeze()]\n",
    "sum(zero_mask) == ParVal.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "ParVal_Short = ParVal[:, torch.nonzero(zero_mask).squeeze()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nonzero(zero_mask).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(ParVal.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
